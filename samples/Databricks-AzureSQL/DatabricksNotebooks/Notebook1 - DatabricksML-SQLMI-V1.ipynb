{"cells":[{"cell_type":"markdown","source":["# Notebook 1\n# ML Prediction of COVID-19 fatalities\nThis notebook extracts data from [COVID-19 Data Lake](https://azure.microsoft.com/en-au/services/open-datasets/catalog/ecdc-covid-19-cases/) published on Microsoft Azure Open Datasets.<br>\nThe dataset is the latest available public data on geographic distribution of COVID-19 cases worldwide from the *European Center for Disease Prevention and Control (ECDC)*. Each row/entry contains the number of new cases reported per day and per country.\nMore information on the dataset and COVID-19 Data Lake can be found [*here*](https://azure.microsoft.com/en-au/services/open-datasets/catalog/ecdc-covid-19-cases/)."],"metadata":{}},{"cell_type":"markdown","source":["Import all the necessary modules used in this notebook for machine learning"],"metadata":{}},{"cell_type":"code","source":["#Load all the necessary pyspark libraries required for this notebook\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.regression import GBTRegressor\nfrom pyspark.ml.feature import VectorIndexer, VectorAssembler, StringIndexer\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import RegressionEvaluator"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["Azure Storage access information that contains the updated Covid-19 cases dataset"],"metadata":{}},{"cell_type":"code","source":["blob_account_name = \"pandemicdatalake\"\nblob_container_name = \"public\"\nblob_relative_path = \"curated/covid-19/ecdc_cases/latest/ecdc_cases.parquet\"\nblob_sas_token = r\"\""],"metadata":{"scrolled":false},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Specify Spark configuration using the information above to access Storage (Blob)"],"metadata":{}},{"cell_type":"code","source":["# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["Read the COVID-19 dataset from Blob and display a sample set"],"metadata":{}},{"cell_type":"code","source":["dfRaw = spark.read.parquet(wasbs_path)\ndisplay(dfRaw)"],"metadata":{"scrolled":true},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["To start preparing the features for ML feature engineering, remove the unnecessary columns from the dataset that might not have any impact on the model itself"],"metadata":{}},{"cell_type":"code","source":["dfClean=dfRaw.drop('geo_id') \\\n              .drop('country_territory_code') \\\n              .drop('continent_exp') \\\n              .drop('load_date') \\\n              .drop('iso_country') \\\n              .drop('date_rep') \\\n              .drop('pop_data_2018')\ndisplay(dfClean)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["The model does not accept string columns - convert the string column representing the country names to an index number that can be a feature in the model"],"metadata":{}},{"cell_type":"code","source":["stringindexer = StringIndexer(inputCol=\"countries_and_territories\", outputCol=\"countries_index\")\ndfTransformed = stringindexer.fit(dfClean).transform(dfClean)\ndisplay(dfTransformed)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["Finalise the feature engineering process by concatenating all feature columns into a feature vector.<br>\nAlso, identify any categorical features to index them."],"metadata":{}},{"cell_type":"code","source":["featuresCols = dfTransformed.columns\nfeaturesCols.remove('deaths')\nfeaturesCols.remove('countries_and_territories')\nvectorAssembler = VectorAssembler(inputCols=featuresCols, outputCol=\"assembledFeatures\", handleInvalid=\"skip\")\n# maxCategories value is set to 210 to consider the countries column as a category (there are roughly 210 countries in the dataset)\nvectorIndexer = VectorIndexer(inputCol=\"assembledFeatures\", outputCol=\"finalFeatures\", maxCategories=210)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["Split the dataset into ***training set* (70%)** and ***test set* (30%)**"],"metadata":{}},{"cell_type":"code","source":["train, test = dfTransformed.randomSplit([0.7, 0.3])\ndisplay(train)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["Train a Gradient Boosted Trees (GBT) model. <br>\n***Note that GBT is chosen in this demo purely for the purpose of an experiment*** and should not be considered as the most effective model for this dataset.\nMore on GBT [*here*](https://en.wikipedia.org/wiki/Gradient_boosting).<br>\nAlso defined here is a grid of hyperparameters to test to get a decent accuracy of the model along with an evaluation metric and a cross validator."],"metadata":{}},{"cell_type":"code","source":["gbt = GBTRegressor(featuresCol=\"finalFeatures\", labelCol=\"deaths\", maxIter=10)\nparamGrid = ParamGridBuilder()\\\n  .addGrid(gbt.maxDepth, [10, 15])\\\n  .addGrid(gbt.maxBins, [210, 250])\\\n  .build()\n# Using the Mean Absolute Error as an evaluation metric\nevaluator = RegressionEvaluator(metricName=\"mae\", labelCol=gbt.getLabelCol(), predictionCol=gbt.getPredictionCol())\n# tune the model using cross validator\ncv = CrossValidator(estimator=gbt, evaluator=evaluator, estimatorParamMaps=paramGrid)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["Create a Pipeline by chaining the assembler, indexer and the gbt cross validator"],"metadata":{}},{"cell_type":"code","source":["pipeline = Pipeline(stages=[vectorAssembler, vectorIndexer, cv])"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["Train the model using the training set"],"metadata":{}},{"cell_type":"code","source":["pipelineModel = pipeline.fit(train)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["Run predictions for the entire dataset"],"metadata":{}},{"cell_type":"code","source":["predictions = pipelineModel.transform(dfTransformed)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["Rename the output columns and convert to appropriate data types in the predicted dataset (to match the schemain Staging Table in SQL MI) before writing the dataset to Azure SQL Managed Instance"],"metadata":{}},{"cell_type":"code","source":["dfPredicted = predictions.select(\"deaths\", \"prediction\", *featuresCols, \"countries_and_territories\") \\\n              .withColumnRenamed(\"pop_data_2018\",\"Population\") \\\n              .withColumnRenamed(\"countries_and_territories\", \"CountryName\") \\\n              .withColumnRenamed(\"countries_index\", \"CountryMLIndex\")\n\ndfPredicted = dfPredicted.withColumn(\"CountryMLIndex\", dfPredicted.CountryMLIndex.cast('integer')) \\\n                .withColumn(\"Day\", dfPredicted.day.cast('integer')) \\\n                .withColumn(\"Month\", dfPredicted.month.cast('integer')) \\\n                .withColumn(\"Year\", dfPredicted.year.cast('integer')) \\\n                .withColumn(\"Cases\", dfPredicted.cases.cast('integer')) \\\n                .withColumn(\"Deaths\", dfPredicted.deaths.cast('integer')) \\\n                .withColumn(\"Prediction\", dfPredicted.prediction.cast('integer'))"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["Use the [SQL Spark connector](https://github.com/microsoft/sql-spark-connector) to write the predicted dataset to staging table \"***dbo.StagingPredictedCovid19***\" in SQL MI.<br>\nThis staging table will be used for further transformation in the subsequent notebook to finalise the data for visualisation."],"metadata":{}},{"cell_type":"code","source":["sqlmiconnection = dbutils.secrets.get(scope = \"sqlmi-kv-secrets\", key = \"sqlmiconn\")\nsqlmiuser = dbutils.secrets.get(scope = \"sqlmi-kv-secrets\", key = \"sqlmiuser\")\nsqlmipwd = dbutils.secrets.get(scope = \"sqlmi-kv-secrets\", key = \"sqlmipwd\")\ndbname = \"Covid19datamart\"\nservername = \"jdbc:sqlserver://\" + sqlmiconnection\ndatabase_name = dbname\nurl = servername + \";\" + \"database_name=\" + dbname + \";\"\ntable_name = \"[Covid19datamart].[dbo].[StagingPredictedCovid19]\"\n\ntry:\n  dfPredicted.write \\\n        .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n        .option(\"url\", url) \\\n        .option(\"dbtable\", table_name) \\\n        .option(\"user\", sqlmiuser) \\\n        .option(\"port\", 3342) \\\n        .option(\"password\", sqlmipwd) \\\n        .option(\"applicationintent\", \"ReadWrite\") \\\n        .mode(\"append\") \\\n        .save()\nexcept ValueError as error :\n    print(\"Connector write failed\", error)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["Connect to Azure SQL MI database to verify the data written in *dbo.StagingPredictedCovid19* <br>\n--End of notebook--"],"metadata":{}}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.7","nbconvert_exporter":"python","file_extension":".py"},"name":"LoadAzureBlobParquet","notebookId":1446400995260376},"nbformat":4,"nbformat_minor":0}
