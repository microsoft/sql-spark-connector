{"cells":[{"cell_type":"markdown","source":["## Notebook 2\n## Read and Write data to Azure SQL Managed Instance\n\nThis notebook emphasizes the connectivity, read and write operations to Azure SQL Managed Instance using the [SQL Spark connector](https://github.com/microsoft/sql-spark-connector).<br> This demo uses a Business Critical SQL MI with a readable secondary replica.\n\n> For the purpose of this demo, the connectivity to SQL MI is\n> established using the **public endpoint**. Port 3342 needs to be specified in this case.<br> For SQL MI connectivity without\n> public endpoints, consider [injecting Databricks into SQL MI\n> VNET](https://docs.microsoft.com/en-us/azure/databricks/administration-guide/cloud-configurations/azure/vnet-inject)\n> or alternatively [connecting with VPN gateway or VNET\n> peering](https://docs.microsoft.com/en-us/azure/azure-sql/managed-instance/connect-application-instance)."],"metadata":{}},{"cell_type":"markdown","source":["Import all the necessary Spark SQL functions used in this notebook for data transformation"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import to_date, concat_ws"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["The SQL MI connection and credential information are stored as secrets in ***Azure Key Vault***. The following snippet retrieves the secrets and stores them in variables.<br>\n> The secrets are always Redacted when displayed in the notebook"],"metadata":{}},{"cell_type":"code","source":["sqlmiconnection = dbutils.secrets.get(scope = \"sqlmi-kv-secrets\", key = \"sqlmiconn\")\nsqlmiuser = dbutils.secrets.get(scope = \"sqlmi-kv-secrets\", key = \"sqlmiuser\")\nsqlmipwd = dbutils.secrets.get(scope = \"sqlmi-kv-secrets\", key = \"sqlmipwd\")\ndbname = \"Covid19datamart\"\nservername = \"jdbc:sqlserver://\" + sqlmiconnection\ndatabase_name = dbname\nurl = servername + \";\" + \"database_name=\" + dbname + \";\""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Read *DimCountry* table from SQL MI's ***ReadOnly replica*** and save it in a dataframe for transformations later in this notebook."],"metadata":{}},{"cell_type":"code","source":["table_name = \"[Covid19datamart].[dbo].[DimCountry]\"\n\ntry:\n  dfCountry = spark.read \\\n        .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n        .option(\"url\", url) \\\n        .option(\"dbtable\", table_name) \\\n        .option(\"applicationintent\", \"ReadOnly\") \\\n        .option(\"user\", sqlmiuser) \\\n        .option(\"port\", 3342) \\\n        .option(\"password\", sqlmipwd).load()\nexcept ValueError as error :\n    print(\"Connector read failed\", error)\n\ndisplay(dfCountry)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["Read *StagingPredictedCovid19* table from SQL MI's ***ReadOnly replica*** and save it in a dataframe for transformations below. <br>This table contains the predicted values from the previous notebook."],"metadata":{}},{"cell_type":"code","source":["table_name = \"[Covid19datamart].[dbo].[StagingPredictedCovid19]\"\n\ntry:\n  dfStagingPredict = spark.read \\\n        .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n        .option(\"url\", url) \\\n        .option(\"dbtable\", table_name) \\\n        .option(\"applicationintent\", \"ReadOnly\") \\\n        .option(\"user\", sqlmiuser) \\\n        .option(\"port\", 3342) \\\n        .option(\"password\", sqlmipwd).load().drop(\"Population\")\nexcept ValueError as error :\n    print(\"Connector read failed\", error)\n\ndisplay(dfStagingPredict)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Join the predicted data with country data (left join) to create a *denormalised dataset* for reporting and visualisation."],"metadata":{}},{"cell_type":"code","source":["dfJoinedPredict = dfStagingPredict.join(dfCountry, dfStagingPredict.CountryName == dfCountry.CountryName,how='left').drop(dfCountry.CountryName)\ndisplay(dfJoinedPredict)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["Include a date column with concatenated values from day, month and year;<br>\nCleanup the final dataset by dropping unwanted columns that are not required for reporting;"],"metadata":{}},{"cell_type":"code","source":["dfResult = dfJoinedPredict.withColumn(\"DateRecorded\", to_date(concat_ws(\"-\",dfJoinedPredict.Year,dfJoinedPredict.Month,dfJoinedPredict.Day)).cast('timestamp')) \\\n  .withColumnRenamed(\"Prediction\",\"PredictedDeaths\") \\\n  .drop(\"CountryMLIndex\") \\\n  .drop(\"LoadDate\") \\\n  .drop(\"DimCountryPK\") \\\n  .drop(\"GeoID\")\n\ndisplay(dfResult)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["Write the final dataset to the table *FactCovid19* in SQL MI.<br>\nNote the *applicationintent* is set to \"*ReadWrite*\" which is also the default when not specified."],"metadata":{}},{"cell_type":"code","source":["table_name = \"[Covid19datamart].[dbo].[FactCovid19]\"\n\ntry:\n  dfResult.write \\\n        .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n        .option(\"url\", url) \\\n        .option(\"dbtable\", table_name) \\\n        .option(\"user\", sqlmiuser) \\\n        .option(\"port\", 3342) \\\n        .option(\"password\", sqlmipwd) \\\n        .option(\"applicationintent\", \"ReadWrite\") \\\n        .mode(\"append\") \\\n        .save()\nexcept ValueError as error :\n    print(\"Connector Write failed\", error)\n"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["display(dfResult)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["-- End of notebook --"],"metadata":{}}],"metadata":{"name":"Notebook2 - Read-Write-SQLMI-V1","notebookId":2876920905179451},"nbformat":4,"nbformat_minor":0}
