{
    "metadata": {
        "kernelspec": {
            "name": "pysparkkernel",
            "display_name": "PySpark",
            "language": ""
        },
        "language_info": {
            "name": "pyspark",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "python",
                "version": 3
            },
            "pygments_lexer": "python3"
        },
        "extensions": {
            "azuredatastudio": {
                "version": 1,
                "views": []
            }
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "code",
            "source": [
                "# spark = SparkSession.builder.getOrCreate()\r\n",
                "# sc.setLogLevel(\"INFO\")\r\n",
                "spark.sparkContext.setLogLevel('DEBUG')\r\n",
                "spark.conf.set('spark.sql.caseSensitive', True)\r\n",
                "\r\n",
                "servername = \"jdbc:sqlserver://master-0.master-svc\"\r\n",
                "dbname = \"connector_test_db\"\r\n",
                "url = servername + \";\" + \"databaseName=\" + dbname + \";\"\r\n",
                "\r\n",
                "user = \"connector_user1\"\r\n",
                "password = \"password123!#\" # Please specify password here"
            ],
            "metadata": {
                "azdata_cell_guid": "fd51c6bb-ca56-47f7-b8ed-62baad1bd782",
                "tags": [],
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                }
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Starting Spark application\n"
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "<IPython.core.display.HTML object>",
                        "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>32</td><td>application_1628887230618_0066</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://10.193.17.239:30443/gateway/default/yarn/proxy/application_1628887230618_0066/\">Link</a></td><td><a target=\"_blank\" href=\"https://10.193.17.239:30443/gateway/default/yarn/container/container_1628887230618_0066_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
                    },
                    "metadata": {}
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "57594e6996e94f2bb9181dee19ddaafa"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "SparkSession available as 'spark'.\n"
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "49029d8d1d164fd0966752ecdf3aa4d5"
                        }
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 2
        },
        {
            "cell_type": "code",
            "source": [
                "data = [(1, \"2020\", \"01\"),\r\n",
                "        (2, \"2020\", \"02\"),\r\n",
                "        (3, \"2020\", \"03\")]\r\n",
                "columns = [\"Id\", \"Year\", \"Month\"]\r\n",
                "df = spark.createDataFrame(data = data, schema = columns)\r\n",
                "df.show()\r\n",
                "\r\n",
                "posts_data = [(1,'Intro','Hi There This is ABC'),\r\n",
                "        (2,'Intro','Hello I''m PQR'),\r\n",
                "        (3,'Re: Intro','Hey PQR This is XYZ'),\r\n",
                "        (4,'Geography','Im George from USA'),\r\n",
                "        (5,'Re:Geography','I''m Mary from OZ'),\r\n",
                "        (6,'Re:Geography','I''m Peter from UK')]\r\n",
                "posts_columns = [\"PostID\", \"PostTitle\", \"PostBody\"]\r\n",
                "posts_df = spark.createDataFrame(data = posts_data, schema = posts_columns)\r\n",
                "posts_df.show()"
            ],
            "metadata": {
                "azdata_cell_guid": "fdc8b9c8-36eb-4280-b32e-6a95b0e11e45",
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                }
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "b0b0e4f5ed664b71aae1a27adc41ddac"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "+---+----+-----+\n| Id|Year|Month|\n+---+----+-----+\n|  1|2020|   01|\n|  2|2020|   02|\n|  3|2020|   03|\n+---+----+-----+\n\n+------+------------+--------------------+\n|PostID|   PostTitle|            PostBody|\n+------+------------+--------------------+\n|     1|       Intro|Hi There This is ABC|\n|     2|       Intro|        Hello Im PQR|\n|     3|   Re: Intro| Hey PQR This is XYZ|\n|     4|   Geography|  Im George from USA|\n|     5|Re:Geography|     Im Mary from OZ|\n|     6|Re:Geography|    Im Peter from UK|\n+------+------------+--------------------+"
                }
            ],
            "execution_count": 5
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **1\\. Write to tables with computed columns**"
            ],
            "metadata": {
                "azdata_cell_guid": "9bd52c1c-fe0c-4f4c-8726-023f3ff642a6"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "## append with computed columns at start and end\r\n",
                "# Write from Spark to SQL table using MSSQL Spark Connector\r\n",
                "dbtable = \"test_computed_col_1\"\r\n",
                "\r\n",
                "try:\r\n",
                "  df.write \\\r\n",
                "    .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\r\n",
                "    .mode(\"append\") \\\r\n",
                "    .option(\"url\", url) \\\r\n",
                "    .option(\"dbtable\", dbtable) \\\r\n",
                "    .option(\"user\", user) \\\r\n",
                "    .option(\"password\", password) \\\r\n",
                "    .option(\"columnsToWrite\", \"Id, Year, Month\") \\\r\n",
                "    .save()\r\n",
                "except ValueError as error :\r\n",
                "    print(\"MSSQL Connector write failed\", error)\r\n",
                "\r\n",
                "print(\"MSSQL Connector write(append) succeeded  \")"
            ],
            "metadata": {
                "azdata_cell_guid": "2f936a04-6110-47d7-ab25-87308576ab57",
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                }
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "ef11011fce0a4d86b2ab67d3506ca6d0"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "MSSQL Connector write(append) succeeded"
                }
            ],
            "execution_count": 6
        },
        {
            "cell_type": "code",
            "source": [
                "#Read from SQL table using MSSQ Connector\r\n",
                "dbtable = \"test_computed_col_1\"\r\n",
                "\r\n",
                "jdbcDF = spark.read \\\r\n",
                "        .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\r\n",
                "        .option(\"url\", url) \\\r\n",
                "        .option(\"dbtable\", dbtable) \\\r\n",
                "        .option(\"user\", user) \\\r\n",
                "        .option(\"password\", password).load()\r\n",
                "\r\n",
                "jdbcDF.show(5)"
            ],
            "metadata": {
                "azdata_cell_guid": "f957925e-2d9a-4cf0-a610-28fcf1e746ba",
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                }
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "3d68cffa23f74740b5c234aec7036761"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "+-------+---+----+-----+--------+\n|   Date| Id|Year|Month|   Years|\n+-------+---+----+-----+--------+\n|2020-02|  2|2020|   02|20202020|\n|2020-03|  3|2020|   03|20202020|\n|2020-01|  1|2020|   01|20202020|\n+-------+---+----+-----+--------+"
                }
            ],
            "execution_count": 7
        },
        {
            "cell_type": "code",
            "source": [
                "## append with 1 computed column in between\r\n",
                "# Write from Spark to SQL table using MSSQL Spark Connector\r\n",
                "dbtable = \"test_computed_col_2\"\r\n",
                "\r\n",
                "try:\r\n",
                "  df.write \\\r\n",
                "    .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\r\n",
                "    .mode(\"append\") \\\r\n",
                "    .option(\"url\", url) \\\r\n",
                "    .option(\"dbtable\", dbtable) \\\r\n",
                "    .option(\"user\", user) \\\r\n",
                "    .option(\"password\", password) \\\r\n",
                "    .option(\"columnsToWrite\", \"Id, Year, Month\") \\\r\n",
                "    .save()\r\n",
                "except ValueError as error :\r\n",
                "    print(\"MSSQL Connector write failed\", error)\r\n",
                "\r\n",
                "print(\"MSSQL Connector write(append) succeeded  \")"
            ],
            "metadata": {
                "azdata_cell_guid": "f51da718-8527-45c0-a739-b0e64af4e7a6",
                "tags": [],
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                }
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "d0f1fb3556824b68acf73d1ef0cc4d6b"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "MSSQL Connector write(append) succeeded"
                }
            ],
            "execution_count": 8
        },
        {
            "cell_type": "code",
            "source": [
                "#Read from SQL table using MSSQL Connector\r\n",
                "dbtable = \"test_computed_col_2\"\r\n",
                "\r\n",
                "jdbcDF = spark.read \\\r\n",
                "        .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\r\n",
                "        .option(\"url\", url) \\\r\n",
                "        .option(\"dbtable\", dbtable) \\\r\n",
                "        .option(\"user\", user) \\\r\n",
                "        .option(\"password\", password).load()\r\n",
                "\r\n",
                "jdbcDF.show(5)"
            ],
            "metadata": {
                "azdata_cell_guid": "bd1b3ccd-3ed7-4b21-9b57-95d1d774d76e",
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                }
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "83f7f1511dd14c26a6b63053fac7a2d3"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "+---+-------+----+-----+\n| Id|   Date|Year|Month|\n+---+-------+----+-----+\n|  1|2020-01|2020|   01|\n|  2|2020-02|2020|   02|\n|  3|2020-03|2020|   03|\n+---+-------+----+-----+"
                }
            ],
            "execution_count": 9
        },
        {
            "cell_type": "code",
            "source": [
                "## append with 2 computed columns in between\r\n",
                "# Write from Spark to SQL table using MSSQL Spark Connector\r\n",
                "dbtable = \"test_computed_col_3\"\r\n",
                "\r\n",
                "try:\r\n",
                "  df.write \\\r\n",
                "    .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\r\n",
                "    .mode(\"append\") \\\r\n",
                "    .option(\"url\", url) \\\r\n",
                "    .option(\"dbtable\", dbtable) \\\r\n",
                "    .option(\"user\", user) \\\r\n",
                "    .option(\"password\", password) \\\r\n",
                "    .option(\"columnsToWrite\", \"Id, Year, Month\") \\\r\n",
                "    .save()\r\n",
                "except ValueError as error :\r\n",
                "    print(\"MSSQL Connector write failed\", error)\r\n",
                "\r\n",
                "print(\"MSSQL Connector write(append) done  \")"
            ],
            "metadata": {
                "azdata_cell_guid": "10bbf920-e50f-4098-be1b-f06024a52fac",
                "tags": [],
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                }
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "880a7b25e6fc472dac6ca47f737ffdda"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "MSSQL Connector write(append) done"
                }
            ],
            "execution_count": 10
        },
        {
            "cell_type": "code",
            "source": [
                "#Read from SQL table using MSSQL Connector\r\n",
                "dbtable = \"test_computed_col_3\"\r\n",
                "\r\n",
                "jdbcDF = spark.read \\\r\n",
                "        .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\r\n",
                "        .option(\"url\", url) \\\r\n",
                "        .option(\"dbtable\", dbtable) \\\r\n",
                "        .option(\"user\", user) \\\r\n",
                "        .option(\"password\", password).load()\r\n",
                "\r\n",
                "jdbcDF.show(5)"
            ],
            "metadata": {
                "azdata_cell_guid": "1c81ee7d-62bc-4a4d-acfa-900b6efeb6ec",
                "tags": [],
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                }
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "94f5c08789b849ffa6e7469d0ccb9e34"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "+---+-------+----+--------+-----+\n| Id|   Date|Year|   Years|Month|\n+---+-------+----+--------+-----+\n|  2|2020-02|2020|20202020|   02|\n|  3|2020-03|2020|20202020|   03|\n|  1|2020-01|2020|20202020|   01|\n+---+-------+----+--------+-----+"
                }
            ],
            "execution_count": 11
        },
        {
            "cell_type": "code",
            "source": [
                "## append with 1 computed column in df and table, but table has 1 less col\r\n",
                "# set schemaCheckEnabled\" as False\r\n",
                "# Write from Spark to SQL table using MSSQL Spark Connector\r\n",
                "dbtable = \"test_computed_col_4\"\r\n",
                "\r\n",
                "try:\r\n",
                "  df.write \\\r\n",
                "    .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\r\n",
                "    .mode(\"append\") \\\r\n",
                "    .option(\"url\", url) \\\r\n",
                "    .option(\"dbtable\", dbtable) \\\r\n",
                "    .option(\"user\", user) \\\r\n",
                "    .option(\"password\", password) \\\r\n",
                "    .option(\"columnsToWrite\", \"Id, Year\") \\\r\n",
                "    .save()\r\n",
                "except ValueError as error :\r\n",
                "    print(\"MSSQL Connector write failed\", error)\r\n",
                "\r\n",
                "print(\"MSSQL Connector write(append) done  \")"
            ],
            "metadata": {
                "azdata_cell_guid": "0fcd38ab-a367-45e7-8d80-acb978176c82",
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                }
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "2bc5b9342d5a4ae8ba5a4f030aedc00e"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "MSSQL Connector write(append) done"
                }
            ],
            "execution_count": 12
        },
        {
            "cell_type": "code",
            "source": [
                "#Read from SQL table using MSSQL Connector\r\n",
                "dbtable = \"test_computed_col_4\"\r\n",
                "\r\n",
                "jdbcDF = spark.read \\\r\n",
                "        .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\r\n",
                "        .option(\"url\", url) \\\r\n",
                "        .option(\"dbtable\", dbtable) \\\r\n",
                "        .option(\"user\", user) \\\r\n",
                "        .option(\"password\", password).load()\r\n",
                "\r\n",
                "jdbcDF.show(5)"
            ],
            "metadata": {
                "azdata_cell_guid": "bb9add17-d962-4e9d-b407-cf1d43cbaf98",
                "extensions": {
                    "azuredatastudio": {
                        "views": []
                    }
                }
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "26d3b2fe1d1e488a9304c40be6991d14"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "+---+----+--------+\n| Id|Year|   Years|\n+---+----+--------+\n|  1|2020|20202020|\n|  2|2020|20202020|\n|  3|2020|20202020|\n+---+----+--------+"
                }
            ],
            "execution_count": 13
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **2\\. Write to Temporal Tables (Generated Always)**"
            ],
            "metadata": {
                "azdata_cell_guid": "1ce0f440-3209-4ee2-9bb0-0fa82f0c55bb"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# temporal table: Write from Spark to SQL table using MSSQL Spark Connector\r\n",
                "dbtable = \"dbo.tempTest\"\r\n",
                "\r\n",
                "try:\r\n",
                "  df.write \\\r\n",
                "    .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\r\n",
                "    .mode(\"append\") \\\r\n",
                "    .option(\"truncate\", \"true\") \\\r\n",
                "    .option(\"url\", url) \\\r\n",
                "    .option(\"dbtable\", dbtable) \\\r\n",
                "    .option(\"user\", user) \\\r\n",
                "    .option(\"password\", password) \\\r\n",
                "    .option(\"tableLock\",True) \\\r\n",
                "    .option(\"columnsToWrite\", \"Id, Year, Month\") \\\r\n",
                "    .save()\r\n",
                "except ValueError as error :\r\n",
                "    print(\"MSSQL Connector write failed\", error)\r\n",
                "\r\n",
                "print(\"MSSQL Connector write(append) succeeded  \")\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "fa16ca86-1837-4613-9650-78eed59eef3e"
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "451e02a8fa0d43cabe0032632b296546"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "MSSQL Connector write(append) succeeded"
                }
            ],
            "execution_count": 15
        },
        {
            "cell_type": "code",
            "source": [
                "#Read from SQL table using MSSQ Connector\r\n",
                "dbtable = \"dbo.tempTest\"\r\n",
                "print(\"read data from SQL server table  \")\r\n",
                "jdbcDF = spark.read \\\r\n",
                "        .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\r\n",
                "        .option(\"url\", url) \\\r\n",
                "        .option(\"dbtable\", dbtable) \\\r\n",
                "        .option(\"user\", user) \\\r\n",
                "        .option(\"password\", password).load()\r\n",
                "\r\n",
                "jdbcDF.show(truncate = False)"
            ],
            "metadata": {
                "azdata_cell_guid": "dcaa3a1b-4770-4a95-80ea-d3e193091ca0"
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "31ec9191e04341a28fb8654c4bf726da"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "read data from SQL server table  \n+---+----+-----+--------------------------+--------------------------+\n|Id |Year|Month|ValidFrom                 |ValidTo                   |\n+---+----+-----+--------------------------+--------------------------+\n|1  |2020|01   |2021-10-25 06:52:27.622804|9999-12-31 23:59:59.999999|\n|2  |2020|02   |2021-10-25 06:52:27.602376|9999-12-31 23:59:59.999999|\n|3  |2020|03   |2021-10-25 06:52:27.602376|9999-12-31 23:59:59.999999|\n+---+----+-----+--------------------------+--------------------------+"
                }
            ],
            "execution_count": 16
        },
        {
            "cell_type": "code",
            "source": [
                "# temporal table: Write from Spark to SQL table using MSSQL Spark Connector\r\n",
                "dbtable = \"test_computed_temp\"\r\n",
                "\r\n",
                "try:\r\n",
                "  df.write \\\r\n",
                "    .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\r\n",
                "    .mode(\"append\") \\\r\n",
                "    .option(\"truncate\", \"true\") \\\r\n",
                "    .option(\"url\", url) \\\r\n",
                "    .option(\"dbtable\", dbtable) \\\r\n",
                "    .option(\"user\", user) \\\r\n",
                "    .option(\"password\", password) \\\r\n",
                "    .option(\"tableLock\",True) \\\r\n",
                "    .option(\"columnsToWrite\", \"Id, Year, Month\") \\\r\n",
                "    .save()\r\n",
                "except ValueError as error :\r\n",
                "    print(\"MSSQL Connector write failed\", error)\r\n",
                "\r\n",
                "print(\"MSSQL Connector write(append) succeeded  \")"
            ],
            "metadata": {
                "azdata_cell_guid": "1283627e-074c-4ff5-8752-b4cdf88b27f3"
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "790a82624c2e4c6681debdc7db2efc1e"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "MSSQL Connector write(append) succeeded"
                }
            ],
            "execution_count": 20
        },
        {
            "cell_type": "code",
            "source": [
                "#Read from SQL table using MSSQ Connector\r\n",
                "dbtable = \"test_computed_temp\"\r\n",
                "print(\"read data from SQL server table  \")\r\n",
                "jdbcDF = spark.read \\\r\n",
                "        .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\r\n",
                "        .option(\"url\", url) \\\r\n",
                "        .option(\"dbtable\", dbtable) \\\r\n",
                "        .option(\"user\", user) \\\r\n",
                "        .option(\"password\", password).load()\r\n",
                "\r\n",
                "jdbcDF.show(truncate = False)"
            ],
            "metadata": {
                "azdata_cell_guid": "982598ad-2c64-4ee7-9ddf-6e1ae6f0d2c5"
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "87ba143688b246ffb9405b65d2cdb7a8"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "read data from SQL server table  \n+-------+---+----+-----+--------+-------------------------+--------------------------+\n|Date   |Id |Year|Month|Years   |ValidFrom                |ValidTo                   |\n+-------+---+----+-----+--------+-------------------------+--------------------------+\n|2020-01|1  |2020|01   |20202020|2021-10-25 07:02:34.68064|9999-12-31 23:59:59.999999|\n|2020-02|2  |2020|02   |20202020|2021-10-25 07:02:34.66426|9999-12-31 23:59:59.999999|\n|2020-03|3  |2020|03   |20202020|2021-10-25 07:02:34.66426|9999-12-31 23:59:59.999999|\n+-------+---+----+-----+--------+-------------------------+--------------------------+"
                }
            ],
            "execution_count": 21
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **3\\. Write to Graph Table**"
            ],
            "metadata": {
                "azdata_cell_guid": "6ccbf927-2f99-44a1-a6d6-21510dbde904"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# Graph table: Write from Spark to SQL table using MSSQL Spark Connector\r\n",
                "dbtable = \"dbo.ForumPosts\"\r\n",
                "\r\n",
                "try:\r\n",
                "  posts_df.write \\\r\n",
                "    .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\r\n",
                "    .mode(\"append\") \\\r\n",
                "    .option(\"truncate\", \"true\") \\\r\n",
                "    .option(\"url\", url) \\\r\n",
                "    .option(\"dbtable\", dbtable) \\\r\n",
                "    .option(\"user\", user) \\\r\n",
                "    .option(\"password\", password) \\\r\n",
                "    .option(\"tableLock\",True) \\\r\n",
                "    .option(\"columnsToWrite\", \"PostID, PostTitle, PostBody\") \\\r\n",
                "    .save()\r\n",
                "except ValueError as error :\r\n",
                "    print(\"MSSQL Connector write failed\", error)\r\n",
                "\r\n",
                "print(\"MSSQL Connector write(append) succeeded  \")"
            ],
            "metadata": {
                "azdata_cell_guid": "1469a88a-58b9-4007-8616-e9c594176b1d"
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "1520e2d0d8294ce992442d4507a0479f"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "MSSQL Connector write(append) succeeded"
                }
            ],
            "execution_count": 23
        },
        {
            "cell_type": "code",
            "source": [
                "#Read from SQL table using MSSQ Connector\r\n",
                "dbtable = \"dbo.ForumPosts\"\r\n",
                "print(\"read data from SQL server table  \")\r\n",
                "jdbcDF = spark.read \\\r\n",
                "        .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\r\n",
                "        .option(\"url\", url) \\\r\n",
                "        .option(\"dbtable\", dbtable) \\\r\n",
                "        .option(\"user\", user) \\\r\n",
                "        .option(\"password\", password).load()\r\n",
                "\r\n",
                "jdbcDF.show(5)"
            ],
            "metadata": {
                "azdata_cell_guid": "3095ba6d-e02f-4a0a-95d1-3e13be55fe20",
                "tags": []
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "a244c97f89df4bb38f6db0e77e3d64ed"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "read data from SQL server table  \n+-----------------------------------------+------+------------+--------------------+\n|$node_id_E3C19E7D3E1648A2BD5364EF445EA866|PostID|   PostTitle|            PostBody|\n+-----------------------------------------+------+------------+--------------------+\n|                     {\"type\":\"node\",\"s...|     1|       Intro|Hi There This is ABC|\n|                     {\"type\":\"node\",\"s...|     2|       Intro|        Hello Im PQR|\n|                     {\"type\":\"node\",\"s...|     3|   Re: Intro| Hey PQR This is XYZ|\n|                     {\"type\":\"node\",\"s...|     4|   Geography|  Im George from USA|\n|                     {\"type\":\"node\",\"s...|     5|Re:Geography|     Im Mary from OZ|\n+-----------------------------------------+------+------------+--------------------+\nonly showing top 5 rows"
                }
            ],
            "execution_count": 24
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **4\\. Write to Tables with Identity columns**"
            ],
            "metadata": {
                "azdata_cell_guid": "3fd6f1f2-cac4-42c1-a3c0-118de963d6ac"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# auto increment columns: Write from Spark to SQL table using MSSQL Spark Connector\r\n",
                "dbtable = \"test_identity\"\r\n",
                "\r\n",
                "try:\r\n",
                "  df.write \\\r\n",
                "    .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\r\n",
                "    .mode(\"append\") \\\r\n",
                "    .option(\"truncate\", \"true\") \\\r\n",
                "    .option(\"url\", url) \\\r\n",
                "    .option(\"dbtable\", dbtable) \\\r\n",
                "    .option(\"user\", user) \\\r\n",
                "    .option(\"password\", password) \\\r\n",
                "    .option(\"tableLock\",True) \\\r\n",
                "    .option(\"columnsToWrite\", \"Year, Month\") \\\r\n",
                "    .save()\r\n",
                "except ValueError as error :\r\n",
                "    print(\"MSSQL Connector write failed\", error)\r\n",
                "\r\n",
                "print(\"MSSQL Connector write(append) succeeded  \")"
            ],
            "metadata": {
                "azdata_cell_guid": "99bf84bc-c6bb-43a9-aa6b-9f3387cbcae1"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "#Read from SQL table using MSSQ Connector\r\n",
                "dbtable = \"test_identity\"\r\n",
                "print(\"read data from SQL server table  \")\r\n",
                "jdbcDF = spark.read \\\r\n",
                "        .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\r\n",
                "        .option(\"url\", url) \\\r\n",
                "        .option(\"dbtable\", dbtable) \\\r\n",
                "        .option(\"user\", user) \\\r\n",
                "        .option(\"password\", password).load()\r\n",
                "\r\n",
                "jdbcDF.show(5)"
            ],
            "metadata": {
                "azdata_cell_guid": "d477c6a5-4e69-4006-8742-786b41fb4031"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **5\\. Write to Table skipping some non-nullable columns**"
            ],
            "metadata": {
                "azdata_cell_guid": "340e9ce8-eabb-4cd5-95e9-ff5b3e6b5b46"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# skip non-nullable column: Write from Spark to SQL table using MSSQL Spark Connector\r\n",
                "dbtable = \"test_skip_cols\"\r\n",
                "\r\n",
                "try:\r\n",
                "  df.write \\\r\n",
                "    .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\r\n",
                "    .mode(\"append\") \\\r\n",
                "    .option(\"truncate\", \"true\") \\\r\n",
                "    .option(\"url\", url) \\\r\n",
                "    .option(\"dbtable\", dbtable) \\\r\n",
                "    .option(\"user\", user) \\\r\n",
                "    .option(\"password\", password) \\\r\n",
                "    .option(\"tableLock\",True) \\\r\n",
                "    .option(\"columnsToWrite\", \"Id, Year\") \\\r\n",
                "    .save()\r\n",
                "except ValueError as error :\r\n",
                "    print(\"MSSQL Connector write failed\", error)\r\n",
                "\r\n",
                "print(\"MSSQL Connector write(append) succeeded  \")"
            ],
            "metadata": {
                "azdata_cell_guid": "98ee8100-f43f-468b-af97-9024144f002c"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "#Read from SQL table using MSSQ Connector\r\n",
                "dbtable = \"test_skip_cols\"\r\n",
                "print(\"read data from SQL server table  \")\r\n",
                "jdbcDF = spark.read \\\r\n",
                "        .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\r\n",
                "        .option(\"url\", url) \\\r\n",
                "        .option(\"dbtable\", dbtable) \\\r\n",
                "        .option(\"user\", user) \\\r\n",
                "        .option(\"password\", password).load()\r\n",
                "\r\n",
                "jdbcDF.show(5)"
            ],
            "metadata": {
                "azdata_cell_guid": "d687eb8f-435d-4ba9-b396-9e756185c4ca"
            },
            "outputs": [],
            "execution_count": null
        }
    ]
}